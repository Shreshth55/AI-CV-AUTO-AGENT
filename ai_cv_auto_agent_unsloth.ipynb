{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec41f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTED to Google Colab!\n",
      "Fri Dec  5 13:42:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"CONNECTED to Google Colab!\")\n",
    "    \n",
    "    # Check for GPU\n",
    "    !nvidia-smi\n",
    "except ImportError:\n",
    "    print(\"NOT Connected. You are running on your local computer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0046a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "%pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f21c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19f0e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.6: Fast Llama patching. Transformers: 4.57.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b44bc8d69b84b35a9ca3324b309de57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf24d3deb4d448db469b8377f323aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2deb6fcec7994f139d28fec7d4933f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de72336226444224a5556254403cf946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1bdacdd8c64a1d9f637f7fac068b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is on this device:cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (2-31): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length=2048\n",
    "load_in_4bit=True\n",
    "dtype=None\n",
    "\n",
    "model_name=\"unsloth/llama-3.1-8b-unsloth-bnb-4bit\"\n",
    "\n",
    "model,tokenizer=FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    dtype=dtype,\n",
    ")\n",
    "print(f\"model is on this device:{model.device}\")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc0dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, max_new_tokens:int=512) -> str:\n",
    "    input_ids=tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to (model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs=model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    full_text=tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aadd4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In one sentence,explain what YOLO does in computer vision. What is the difference between YOLO and R-CNN?\n",
      "YOLO is a deep learning algorithm that is used for object detection in images. It is a single-shot detector, meaning that it can detect objects in an image in a single pass through the network, without the need for multiple passes or iterations. This makes it faster and more efficient than other object detection algorithms.\n",
      "YOLO is based on the idea of predicting the bounding boxes and class probabilities for all objects in an image at once, rather than predicting the bounding boxes and class probabilities for each object separately. This allows YOLO to be more efficient and faster than other object detection algorithms.\n",
      "The main difference between YOLO and R-CNN is that YOLO is a single-shot detector, while R-CNN is a multi-stage detector. R-CNN uses a two-stage approach, where it first proposes regions of interest (ROIs) and then classifies them. YOLO, on the other hand, uses a single-stage approach, where it predicts the bounding boxes and class probabilities for all objects in an image at once.\n",
      "Another difference between YOLO and R-CNN is that YOLO is trained using a loss function that is based on the intersection-over-union (IoU) between the predicted bounding boxes and the ground truth bounding boxes. R-CNN, on the other hand, is trained using a loss function that is based on the classification accuracy of the predicted bounding boxes.\n",
      "Overall, YOLO is a faster and more efficient object detection algorithm than R-CNN, and it is particularly useful for real-time applications where speed and efficiency are important.\n",
      "In one sentence,explain what YOLO does in computer vision. What is the difference between YOLO and R-CNN?\n",
      "YOLO is a deep learning algorithm that is used for object detection in images. It is a single-shot detector, meaning that it can detect objects in an image in a single pass through the network, without the need for multiple passes or iterations. This makes it faster and more efficient than other object detection algorithms.\n",
      "YOLO is based on the idea of predicting the bounding boxes and class probabilities for all objects in an image at once, rather than predicting the bounding boxes and class probabilities for each object separately. This allows YOLO to be more efficient and faster than other object detection algorithms.\n",
      "The main difference between YOLO and R-CNN is that YOLO is a single-shot detector, while R-CNN is a multi-stage detector. R-CNN uses a two\n"
     ]
    }
   ],
   "source": [
    "print(call_llm(\"In one sentence,explain what YOLO does in computer vision.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8026bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_prompt(task_description: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an expert computer vision data scientist.\n",
    "\n",
    "Task:\n",
    "{task_description}\n",
    "\n",
    "Design a DATASET PLAN for this task. Include:\n",
    "\n",
    "1. Classes (with short explanations).\n",
    "2. Recommended data sources (e.g., Roboflow keywords or types of images).\n",
    "3. Approximate number of images per class for an MVP and for a stronger model.\n",
    "4. Train/validation/test split percentages.\n",
    "5. Recommended data augmentations (and why).\n",
    "6. Notes specific to CCTV-style footage and phone usage.\n",
    "\n",
    "Format your answer as clear Markdown with headings and bullet points.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def yolo_config_prompt(task_description: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a senior YOLO engineer.\n",
    "\n",
    "Task:\n",
    "{task_description}\n",
    "\n",
    "Design a YOLO training CONFIGURATION including:\n",
    "\n",
    "1. Model variant (e.g., 'yolov8s', 'yolov8m') and justify the choice.\n",
    "2. Image size (imgsz).\n",
    "3. Batch size for a Colab T4 GPU.\n",
    "4. Number of epochs for a first run, and for a more refined run.\n",
    "5. Important hyperparameters (learning rate, optimizer, augmentations settings).\n",
    "6. Any recommended YOLO training tricks (cosine LR, warmup, etc.).\n",
    "\n",
    "Output:\n",
    "First, give a YAML-style block with keys like:\n",
    "- model\n",
    "- imgsz\n",
    "- batch\n",
    "- epochs\n",
    "- lr0\n",
    "- optimizer\n",
    "etc.\n",
    "\n",
    "Then give a short explanation in Markdown.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def training_script_prompt(task_description: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a Python + ML engineer.\n",
    "\n",
    "Task:\n",
    "{task_description}\n",
    "\n",
    "Write a PYTHON TRAINING SCRIPT OUTLINE for training a YOLO model on a custom dataset in YOLO format on Google Colab.\n",
    "\n",
    "Requirements:\n",
    "- Assume we use the 'ultralytics' package.\n",
    "- Assume a dataset YAML path: './data.yaml'.\n",
    "- The script should:\n",
    "  - Set model name (e.g. 'yolov8s.pt') via a variable.\n",
    "  - Define basic arguments: epochs, imgsz, batch.\n",
    "  - Load the model.\n",
    "  - Train the model.\n",
    "  - Save best weights.\n",
    "  - Run validation and print mAP.\n",
    "- This is an outline, not perfect final code, but should be runnable with small edits.\n",
    "\n",
    "Return ONLY plain Python code (no backticks, no Markdown).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def onnx_tensorrt_prompt(task_description: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an NVIDIA TensorRT engineer.\n",
    "\n",
    "Task:\n",
    "{task_description}\n",
    "\n",
    "Design an ONNX + TensorRT OPTIMIZATION PLAN for a trained YOLO model.\n",
    "\n",
    "Include:\n",
    "\n",
    "1. How to export the YOLO model to ONNX (CLI or Python).\n",
    "2. How to convert the ONNX model to a TensorRT engine (e.g., using 'trtexec').\n",
    "3. How to benchmark:\n",
    "   - Latency per frame (ms)\n",
    "   - FPS\n",
    "   - GPU memory usage\n",
    "4. How to use 'nvidia-smi' to monitor GPU utilization while running inference.\n",
    "5. Suggestions for FP16 / INT8 optimization tradeoffs.\n",
    "\n",
    "Format as Markdown with numbered steps, code blocks (CLI or Python), and bullet points.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def readme_prompt(task_description: str, project_name: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a senior MLOps engineer writing a GitHub README for a portfolio project.\n",
    "\n",
    "Project name: {project_name}\n",
    "\n",
    "Task:\n",
    "{task_description}\n",
    "\n",
    "Write a README.md including:\n",
    "\n",
    "1. Title and one-line description.\n",
    "2. Problem statement and why this CV task matters.\n",
    "3. High-level architecture as a bullet flow.\n",
    "4. Tech stack (YOLO, ONNX, TensorRT, Unsloth LLM Planner).\n",
    "5. How the AI CV Auto-Agent works:\n",
    "   - input (natural language task)\n",
    "   - planner (LLM)\n",
    "   - generated artifacts (dataset plan, config, script, optimization plan)\n",
    "   - user training & deployment.\n",
    "6. How to:\n",
    "   - Set up environment (Colab).\n",
    "   - Run the planner to generate a project.\n",
    "   - Train the YOLO model using the generated script.\n",
    "   - (Optionally) run ONNX/TensorRT optimization.\n",
    "7. Section for 'Example Task: Phone Usage Detection in CCTV'.\n",
    "8. Future improvements (better planner, more tasks, automatic training, etc).\n",
    "\n",
    "Format in Markdown with headings.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eab707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GeneratedProject:\n",
    "    dataset_plan: str\n",
    "    yolo_config: str\n",
    "    training_code: str\n",
    "    onnx_tensorrt_plan: str\n",
    "    readme_md: str\n",
    "\n",
    "\n",
    "def generate_project_artifacts(task_description: str, project_name: str) -> GeneratedProject:\n",
    "    print(\"Generating dataset plan...\")\n",
    "    ds = call_llm(dataset_prompt(task_description))\n",
    "\n",
    "    print(\"Generating YOLO config...\")\n",
    "    yc = call_llm(yolo_config_prompt(task_description))\n",
    "\n",
    "    print(\"Generating training script outline...\")\n",
    "    tc = call_llm(training_script_prompt(task_description), max_new_tokens=700)\n",
    "\n",
    "    print(\"Generating ONNX + TensorRT optimization plan...\")\n",
    "    ot = call_llm(onnx_tensorrt_prompt(task_description), max_new_tokens=700)\n",
    "\n",
    "    print(\"Generating README...\")\n",
    "    rm = call_llm(readme_prompt(task_description, project_name), max_new_tokens=700)\n",
    "\n",
    "    return GeneratedProject(\n",
    "        dataset_plan=ds,\n",
    "        yolo_config=yc,\n",
    "        training_code=tc,\n",
    "        onnx_tensorrt_plan=ot,\n",
    "        readme_md=rm,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a9170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset plan...\n",
      "Generating YOLO config...\n",
      "Generating training script outline...\n",
      "Generating ONNX + TensorRT optimization plan...\n",
      "Generating README...\n",
      "Generated project at: /content/generated_projects/phone-usage-detection-in-cctv\n",
      "Files:\n",
      " - train_yolo.py\n",
      " - yolo_config.yaml\n",
      " - ONNX_TENSORRT_PLAN.md\n",
      " - README.md\n",
      " - DATASET_PLAN.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "PROJECTS_DIR = BASE_DIR / \"generated_projects\"\n",
    "PROJECTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \"-\", text)\n",
    "    text = re.sub(r\"-+\", \"-\", text)\n",
    "    return text.strip(\"-\")\n",
    "\n",
    "\n",
    "task_description = \"Detect phone usage (person using a mobile phone) in indoor CCTV footage, such as offices or factories.\"\n",
    "project_name = \"Phone Usage Detection in CCTV\"\n",
    "\n",
    "artifacts = generate_project_artifacts(task_description, project_name)\n",
    "\n",
    "slug = slugify(project_name)\n",
    "project_path = PROJECTS_DIR / slug\n",
    "project_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Write files\n",
    "(project_path / \"DATASET_PLAN.md\").write_text(artifacts.dataset_plan, encoding=\"utf-8\")\n",
    "(project_path / \"yolo_config.yaml\").write_text(artifacts.yolo_config, encoding=\"utf-8\")\n",
    "(project_path / \"train_yolo.py\").write_text(artifacts.training_code, encoding=\"utf-8\")\n",
    "(project_path / \"ONNX_TENSORRT_PLAN.md\").write_text(artifacts.onnx_tensorrt_plan, encoding=\"utf-8\")\n",
    "(project_path / \"README.md\").write_text(artifacts.readme_md, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Generated project at:\", project_path)\n",
    "print(\"Files:\")\n",
    "for f in project_path.iterdir():\n",
    "    print(\" -\", f.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0199ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
