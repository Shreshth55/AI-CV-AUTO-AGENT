{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ec41f835",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec41f835",
        "outputId": "23d8b0c1-456c-4a5b-9a66-7b6900733656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CONNECTED to Google Colab!\n",
            "Sat Dec  6 18:48:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Run this cell\n",
        "try:\n",
        "    import google.colab\n",
        "    print(\"CONNECTED to Google Colab!\")\n",
        "\n",
        "    # Check for GPU\n",
        "    !nvidia-smi\n",
        "except ImportError:\n",
        "    print(\"NOT Connected. You are running on your local computer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0046a790",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0046a790",
        "outputId": "8694f36a-e620-457b-a5ba-858aefbd63d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.0/288.0 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\" -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f2f21c67",
      "metadata": {
        "id": "f2f21c67"
      },
      "outputs": [],
      "source": [
        "%pip install transformers accelerate datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d19f0e1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429,
          "referenced_widgets": [
            "8ecaa0c4378a4b779e2d9961c587ba86",
            "53a57621a41541aba1cae5b9fdfb1164",
            "bc48e3a24b6c4695bc141578c2dd5d90",
            "685ee47dd5a844a2a09d384c087381d1",
            "b4ec79cf4df54f0d84ed288c58d658de",
            "53120372438a4c64a06192c2d439ce9e",
            "554d56674a0043ffa40f33154e5a0d42",
            "e5dbd77e4ea74da8af3594ce4c125f99",
            "3429cbd9fcba479dbf88891ad689d6ae",
            "c6a338fd243b4467aa5f710a7c7652c6",
            "25b211f951e64ef3bffb3b43704ac3dc",
            "abf7732c66f44dffb9cb14d9b91147f2",
            "3eea8f2c84164219ba4e4245384d2d3a",
            "0b5e7b46103c48089650390b4f38e3f5",
            "83d98412bafd4aa28bf8a4f72db472bb",
            "476d90cb98514571b9e9d52b94b08cc4",
            "e5ffbcd9e0af40f592bc37c2248cd481",
            "b0dccd476c294ff69f718f42ae8ede86",
            "6183b8f9e5da43dbbb357dac4825efe8",
            "7a1e636797e94a08930b1458242531cb",
            "59296e0fa8c644bf8736d6d9ffbf3109",
            "86bb217eddb446e3b4b572b8f056fa4f",
            "d76caa47267f42bb9cf4b93766078d1e",
            "946f43adfae542ad940299f5d3355e78",
            "fc8f24bcd91b48c1b0008c779c558de3",
            "e0565b98b54c4ae58a7a60404511f176",
            "f35e2fbbaa974009ab9b2334c255fe27",
            "39bb1494513e42a5ab9b05ca53ae71f9",
            "08f26555ba3f4b3a84a02187c490e91c",
            "b8d66a6a64054321883d4505f262025d",
            "ffb36381be6146079cef1ca9abff4ecb",
            "7fe6d9b29347453cadd23e643152f362",
            "a4f94aa19ba7449cba4c55c33d648b76",
            "5e85691d57ff497c941bcb6e81b8c0f9",
            "0c663c80a78e4f1794fed76641074bd8",
            "4284947a67ab439cb4c5560eaaef8e97",
            "7324d8e312ad4a70a67614de699dc752",
            "6b3d15adfe8e486c9174dcc8de2ac85c",
            "a553ebc2636e45a9a9c153bc7034100c",
            "b9a497da980e4f57befe27b9d6ff5f2a",
            "08e806965eba493ea26e27dc6c2a15d9",
            "48817e7aefab4a9ea7fd609b037b4cc3",
            "76b90a79a8414f968b6d1ad31c8334c8",
            "b575d3559c3d437392d27bffe7a3e98a",
            "62d047e7899240e0a428924175e85bbb",
            "7312b9843d444722a98ae95c4b210635",
            "1e7cb6d8e45046498b74000be45f7731",
            "8f3bf5acb7684e5590d904a1f09312fc",
            "e87ccb3125f14a1ea88ca3a5c01e2ff2",
            "aa534ce7c2504bf4beaceaf75151705e",
            "76171d755abf4912a0fed18909adbe6b",
            "07623615dd044c928d0ebf8b99fdfd83",
            "0f84b30be8c64fbf88ee913e98d0f6db",
            "daf6ebdf28424fa98b8fbe7b79d33fce",
            "5c2e56a3aab04f6ba3074d77b72b5e99",
            "0c8ab425a024447e96dcbcc279071541",
            "bbb29ec9cb944e17a984df619f235a0b",
            "9c89fedf82f44e4daf724b6d2b661d70",
            "73ccacd04f0e4021aa09297950af4e4b",
            "f9931ac209f6425ea64076e38daae848",
            "e55b42aef6fd43619508caccb1b104fd",
            "1d95e53a082c4cea8a3bb5b7130a6ef1",
            "7a8dcf997f85445482bc26640a30ec01",
            "9c1663016db14fd0ae1c20ec48fa1f00",
            "3859d0b8054e4a79a4cd412e91c81b25",
            "2b6014492e4c49a69519dbed2303c05b",
            "7d2daccf33234ea197f3401bc717dadf",
            "e7563ddde3504c8f92951a1d489cd282",
            "611e7cc4773c4a3692d695d4153206db",
            "8b53ffa19d694d6c8c9d467db4d654b9",
            "1a318cc91e7a4fe39cb8c6f81673b49f",
            "23a09ba55dad44a98d40122d5e9e9588",
            "973278027c7947b69082583697fdaa30",
            "480e803f1e7f461a8c47a0bd9ab8a9ab",
            "81cb8fc38eb84e1bb064066732be14d7",
            "e1c8aed4bce84e3bb905a8feef382907",
            "76e726a7793a44ad91a615acf69ce28a",
            "6c77843c48334ebcae0dba4be78e7bf7",
            "cd9fd52a1ceb46bea4f41d6123bcd0f0",
            "4d2dc835afd64543a232e84a8518496a",
            "5e614d4ea587444bb649ac1ec7fe07fd",
            "60e49294803d4bcb83067d2208f38bbb",
            "5d4ab9a652ed4dc19fffc1a498dfc2e8",
            "16bc90cdcacd4a6caf107a3068ca574d",
            "d9cb68e820db49a8aed6b1c11ce8080c",
            "410b6efcaf1c4240a615db6355d0ba96",
            "7a23965398634b30a7bd9330d30e3cfe",
            "beae3d90ab244f0bba1858a098822906"
          ]
        },
        "id": "d19f0e1b",
        "outputId": "52c1ff7f-fab2-4827-bd3c-a91ba7b81bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.11.6: Fast Qwen2 patching. Transformers: 4.57.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2047102f49e64941af1abf30972eff85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cba14e8c91b40b4b4f91acae22a9a31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "427f8155556a4933957e07c43234c961",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6852f93ff46e45b5b6075a37371eecc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abdb863b28ed4292bf6bced8c0aba821",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95400d737bf04cb0a704ee460f31e1cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59bf31ae33fd4418899d521fa901084f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b62200e0cfae48c5932b77a79ada3a0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model is on this device:cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length=2048\n",
        "load_in_4bit=True\n",
        "dtype=None\n",
        "\n",
        "model_name=\"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\"\n",
        "\n",
        "model,tokenizer=FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    dtype=dtype,\n",
        ")\n",
        "model=FastLanguageModel.for_inference(model)\n",
        "print(f\"model is on this device:{model.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc0dfc4",
      "metadata": {
        "id": "bbc0dfc4"
      },
      "outputs": [],
      "source": [
        "def call_llm(prompt: str, max_new_tokens:int=512) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens = max_new_tokens,\n",
        "            do_sample = False,\n",
        "        )\n",
        "\n",
        "    full_text=tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
        "\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4aadd4df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aadd4df",
        "outputId": "630c0d1c-db5c-4faa-f408-3a82727cca83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In one sentence,explain what YOLO does in computer vision. YOLO (You Only Look Once) is a deep learning algorithm that can detect objects in images or videos by training on large datasets and using convolutional neural networks to extract features from the input data.\n",
            "Y0L0\n",
            "YOLO stands for You Only Look Once, which is an object detection algorithm developed by Joseph Redmon et al. It uses a single forward pass of a CNN to predict bounding boxes and class scores simultaneously, making it faster than other methods like Faster R-CNN. The algorithm has been widely used in various applications such as image classification, object detection, and tracking. Its simplicity and efficiency have made it popular among researchers and practitioners in the field of computer vision. Y0L0\n",
            "YOLO\n",
            "YOLO stands for You Only Look Once, which is a real-time object detection system designed to be fast and efficient. It works by predicting bounding boxes and class probabilities at once through a single forward pass of a Convolutional Neural Network (CNN). This approach significantly reduces computational complexity compared to previous methods like Faster R-CNN, allowing YOLO to process images quickly while still achieving high accuracy. Y0L0\n",
            "YOLO\n",
            "YOLO stands for You Only Look Once, which is a state-of-the-art object detection algorithm that achieves real-time performance with minimal computation cost. Developed by Joseph Redmon et al., YOLO utilizes a single forward pass of a CNN to predict bounding boxes and class probabilities simultaneously. By doing so, YOLO efficiently processes images without sacrificing accuracy, making it particularly useful in applications requiring rapid response times. Y0L0\n",
            "YOLO\n",
            "YOLO stands for You Only Look Once, a revolutionary object detection method that revolutionizes the field of computer vision. Designed by Joseph Redmon et al., YOLO employs a unique architecture that allows it to perform object detection in real-time with minimal computational resources. By processing images through a single forward pass of a CNN, YOLO achieves remarkable speed and efficiency, enabling its use in a wide range of applications including autonomous vehicles, surveillance systems, and robotics. Y0L0\n",
            "YOLO\n",
            "YOLO stands for You Only Look Once, a groundbreaking deep learning framework that enables ultra-fast object detection. Developed by Joseph Redmon et al., YOLO leverages a single forward pass of a CNN to predict both bounding boxes and class probabilities simultaneously. This innovative approach drastically reduces computational costs, making it highly effective for real-time applications. Y0L0\n",
            "YOLO\n",
            "YOLO stands for You Only Look Once, a powerful\n"
          ]
        }
      ],
      "source": [
        "print(call_llm(\"In one sentence,explain what YOLO does in computer vision.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8026bfc0",
      "metadata": {
        "id": "8026bfc0"
      },
      "outputs": [],
      "source": [
        "def dataset_prompt(task_description: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are an expert computer vision data scientist.\n",
        "\n",
        "Task:\n",
        "{task_description}\n",
        "\n",
        "Design a DATASET PLAN for this task. Include:\n",
        "\n",
        "1. Classes (with short explanations).\n",
        "2. Recommended data sources (e.g., Roboflow keywords or types of images).\n",
        "3. Approximate number of images per class for an MVP and for a stronger model.\n",
        "4. Train/validation/test split percentages.\n",
        "5. Recommended data augmentations (and why).\n",
        "6. Notes specific to CCTV-style footage and phone usage.\n",
        "\n",
        "Format your answer as clear Markdown with headings and bullet points.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def yolo_config_prompt(task_description: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are a senior YOLO engineer.\n",
        "\n",
        "Task:\n",
        "{task_description}\n",
        "\n",
        "Design a YOLO training CONFIGURATION including:\n",
        "\n",
        "1. Model variant (e.g., 'yolov8s', 'yolov8m') and justify the choice.\n",
        "2. Image size (imgsz).\n",
        "3. Batch size for a Colab T4 GPU.\n",
        "4. Number of epochs for a first run, and for a more refined run.\n",
        "5. Important hyperparameters (learning rate, optimizer, augmentations settings).\n",
        "6. Any recommended YOLO training tricks (cosine LR, warmup, etc.).\n",
        "\n",
        "Output:\n",
        "First, give a YAML-style block with keys like:\n",
        "- model\n",
        "- imgsz\n",
        "- batch\n",
        "- epochs\n",
        "- lr0\n",
        "- optimizer\n",
        "etc.\n",
        "\n",
        "Then give a short explanation in Markdown.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def training_script_prompt(task_description: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are a Python + ML engineer.\n",
        "\n",
        "Task:\n",
        "{task_description}\n",
        "\n",
        "Write a PYTHON TRAINING SCRIPT OUTLINE for training a YOLO model on a custom dataset in YOLO format on Google Colab.\n",
        "\n",
        "Requirements:\n",
        "- Assume we use the 'ultralytics' package.\n",
        "- Assume a dataset YAML path: './data.yaml'.\n",
        "- The script should:\n",
        "  - Set model name (e.g. 'yolov8s.pt') via a variable.\n",
        "  - Define basic arguments: epochs, imgsz, batch.\n",
        "  - Load the model.\n",
        "  - Train the model.\n",
        "  - Save best weights.\n",
        "  - Run validation and print mAP.\n",
        "- This is an outline, not perfect final code, but should be runnable with small edits.\n",
        "\n",
        "Return ONLY plain Python code (no backticks, no Markdown).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def onnx_tensorrt_prompt(task_description: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are an NVIDIA TensorRT engineer.\n",
        "\n",
        "Task:\n",
        "{task_description}\n",
        "\n",
        "Design an ONNX + TensorRT OPTIMIZATION PLAN for a trained YOLO model.\n",
        "\n",
        "Include:\n",
        "\n",
        "1. How to export the YOLO model to ONNX (CLI or Python).\n",
        "2. How to convert the ONNX model to a TensorRT engine (e.g., using 'trtexec').\n",
        "3. How to benchmark:\n",
        "   - Latency per frame (ms)\n",
        "   - FPS\n",
        "   - GPU memory usage\n",
        "4. How to use 'nvidia-smi' to monitor GPU utilization while running inference.\n",
        "5. Suggestions for FP16 / INT8 optimization tradeoffs.\n",
        "\n",
        "Format as Markdown with numbered steps, code blocks (CLI or Python), and bullet points.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def readme_prompt(task_description: str, project_name: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are a senior MLOps engineer writing a GitHub README for a portfolio project.\n",
        "\n",
        "Project name: {project_name}\n",
        "\n",
        "Task:\n",
        "{task_description}\n",
        "\n",
        "Write a README.md including:\n",
        "\n",
        "1. Title and one-line description.\n",
        "2. Problem statement and why this CV task matters.\n",
        "3. High-level architecture as a bullet flow.\n",
        "4. Tech stack (YOLO, ONNX, TensorRT, Unsloth LLM Planner).\n",
        "5. How the AI CV Auto-Agent works:\n",
        "   - input (natural language task)\n",
        "   - planner (LLM)\n",
        "   - generated artifacts (dataset plan, config, script, optimization plan)\n",
        "   - user training & deployment.\n",
        "6. How to:\n",
        "   - Set up environment (Colab).\n",
        "   - Run the planner to generate a project.\n",
        "   - Train the YOLO model using the generated script.\n",
        "   - (Optionally) run ONNX/TensorRT optimization.\n",
        "7. Section for 'Example Task: Phone Usage Detection in CCTV'.\n",
        "8. Future improvements (better planner, more tasks, automatic training, etc).\n",
        "\n",
        "Format in Markdown with headings.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0eab707e",
      "metadata": {
        "id": "0eab707e"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GeneratedProject:\n",
        "    dataset_plan: str\n",
        "    yolo_config: str\n",
        "    training_code: str\n",
        "    onnx_tensorrt_plan: str\n",
        "    readme_md: str\n",
        "\n",
        "\n",
        "def generate_project_artifacts(task_description: str, project_name: str) -> GeneratedProject:\n",
        "    print(\"Generating dataset plan...\")\n",
        "    ds = call_llm(dataset_prompt(task_description))\n",
        "\n",
        "    print(\"Generating YOLO config...\")\n",
        "    yc = call_llm(yolo_config_prompt(task_description))\n",
        "\n",
        "    print(\"Generating training script outline...\")\n",
        "    tc = call_llm(training_script_prompt(task_description), max_new_tokens=700)\n",
        "\n",
        "    print(\"Generating ONNX + TensorRT optimization plan...\")\n",
        "    ot = call_llm(onnx_tensorrt_prompt(task_description), max_new_tokens=700)\n",
        "\n",
        "    print(\"Generating README...\")\n",
        "    rm = call_llm(readme_prompt(task_description, project_name), max_new_tokens=700)\n",
        "\n",
        "    return GeneratedProject(\n",
        "        dataset_plan=ds,\n",
        "        yolo_config=yc,\n",
        "        training_code=tc,\n",
        "        onnx_tensorrt_plan=ot,\n",
        "        readme_md=rm,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "51a9170f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51a9170f",
        "outputId": "fcb7d06a-8d6b-4a1c-fb65-ed0b23a3f151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating dataset plan...\n",
            "Generating YOLO config...\n",
            "Generating training script outline...\n",
            "Generating ONNX + TensorRT optimization plan...\n",
            "Generating README...\n",
            "Generated project at: /content/generated_projects/phone-usage-detection-in-cctv\n",
            "Files:\n",
            " - yolo_config.yaml\n",
            " - DATASET_PLAN.md\n",
            " - ONNX_TENSORRT_PLAN.md\n",
            " - train_yolo.py\n",
            " - README.md\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "PROJECTS_DIR = BASE_DIR / \"generated_projects\"\n",
        "PROJECTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def slugify(text: str) -> str:\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r\"[^a-z0-9]+\", \"-\", text)\n",
        "    text = re.sub(r\"-+\", \"-\", text)\n",
        "    return text.strip(\"-\")\n",
        "\n",
        "\n",
        "task_description = \"Detect phone usage (person using a mobile phone) in indoor CCTV footage, such as offices or factories.\"\n",
        "project_name = \"Phone Usage Detection in CCTV\"\n",
        "\n",
        "artifacts = generate_project_artifacts(task_description, project_name)\n",
        "\n",
        "slug = slugify(project_name)\n",
        "project_path = PROJECTS_DIR / slug\n",
        "project_path.mkdir(exist_ok=True)\n",
        "\n",
        "# Write files\n",
        "(project_path / \"DATASET_PLAN.md\").write_text(artifacts.dataset_plan, encoding=\"utf-8\")\n",
        "(project_path / \"yolo_config.yaml\").write_text(artifacts.yolo_config, encoding=\"utf-8\")\n",
        "(project_path / \"train_yolo.py\").write_text(artifacts.training_code, encoding=\"utf-8\")\n",
        "(project_path / \"ONNX_TENSORRT_PLAN.md\").write_text(artifacts.onnx_tensorrt_plan, encoding=\"utf-8\")\n",
        "(project_path / \"README.md\").write_text(artifacts.readme_md, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Generated project at:\", project_path)\n",
        "print(\"Files:\")\n",
        "for f in project_path.iterdir():\n",
        "    print(\" -\", f.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0199ec9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0199ec9",
        "outputId": "205c85c3-9c11-4e9f-d425-6391a9c0ea08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "E4C20B9sxNSU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "E4C20B9sxNSU",
        "outputId": "6ae66d02-0090-4a34-84dd-50c1b3679e0e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_0e8f23f4-05ab-4789-88d7-b6c3c6da9c4a\", \"generated_projects_archive.zip\", 8090)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "folder_to_download = \"/content/generated_projects\"\n",
        "output_filename = \"generated_projects_archive\"\n",
        "\n",
        "# Create a zip archive of the folder\n",
        "shutil.make_archive(output_filename, 'zip', folder_to_download)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(f\"{output_filename}.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
